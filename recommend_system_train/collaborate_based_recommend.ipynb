{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcMWpHSMQWqf"
      },
      "source": [
        "# Collaboration based recommender system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dbk6-eUTUiz",
        "outputId": "30dc2851-c775-40df-b874-a365128d200a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.11.4)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp310-cp310-linux_x86_64.whl size=2357256 sha256=a406f36664b14cc7bb02c0c63e1842d7c770eef80ecfeed42fa4864d0dbb540f\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/3f/df/6acbf0a40397d9bf3ff97f582cc22fb9ce66adde75bc71fd54\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.4\n"
          ]
        }
      ],
      "source": [
        " !pip install scikit-surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXpakAs5QO8L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "books_df = pd.read_csv('data/books_enriched.csv',  index_col=[0], converters={\"genres\": literal_eval})\n",
        "ratings = pd.read_csv('data/ratings.csv', converters={\"genres\": literal_eval})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUCcNm9jTHnu"
      },
      "outputs": [],
      "source": [
        "\n",
        "book_cols= ['book_id', 'title','description']\n",
        "books_df = books_df.dropna(subset=['description'])\n",
        "books_df = books_df[book_cols]\n",
        "book_rating = ratings.merge(books_df, on='book_id')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac0dbh0-TKbr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUd6Whp2UJl1",
        "outputId": "17df6d43-4925-41d0-eccc-ddb2e7a8233c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7f89b0380070>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "reader = Reader(rating_scale=(0, 5))\n",
        "data = Dataset.load_from_df(book_rating[['user_id', 'book_id', 'rating']], reader)\n",
        "\n",
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "\n",
        "\n",
        "algo = SVD()\n",
        "algo.fit(trainset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JB6pZsWQV7l"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "class CollaborativeRecommender:\n",
        "    def __init__(self, algo, trainset, testset):\n",
        "        self.algo = algo\n",
        "        self.trainset = trainset\n",
        "        self.testset = testset\n",
        "\n",
        "    def predict(self, user_id, k=10):\n",
        "        try:\n",
        "            # Convert raw user_id to inner user_id\n",
        "            inner_user_id = self.trainset.to_inner_uid(user_id)\n",
        "        except ValueError:\n",
        "            # User not found in trainset, return None\n",
        "            return None\n",
        "\n",
        "        # Get the list of books the user has interacted with\n",
        "        user_books = set(self.trainset.ur[inner_user_id])\n",
        "        all_books = set(self.trainset.all_items())\n",
        "        unseen_books = all_books - user_books\n",
        "        # Predict the ratings for unseen books\n",
        "        predictions = [self.algo.predict(self.trainset.to_raw_uid(inner_user_id), self.trainset.to_raw_iid(book_id)) for book_id in unseen_books]\n",
        "        # Sort the predictions by estimated rating and return the top-k books\n",
        "        top_predictions = sorted(predictions, key=lambda x: x.est, reverse=True)[:k]\n",
        "        top_books = [pred.iid for pred in top_predictions]\n",
        "        return top_books\n",
        "    def evaluate(self, k=10):\n",
        "        predictions = self.algo.test(self.testset)\n",
        "        hit_rate, ndcg_scores = [], []\n",
        "        count=0\n",
        "        for user_id in set(pred.uid for pred in predictions):\n",
        "            try:\n",
        "                inner_user_id = self.trainset.to_inner_uid(user_id)\n",
        "            except ValueError:\n",
        "                pred_books = None\n",
        "            else:\n",
        "                pred_books = set(self.predict(user_id, k))\n",
        "\n",
        "            true_books = set(book_id for (uid, book_id, _) in self.testset if uid == user_id)\n",
        "            #print(\"true book {}\".format(len(true_books)))\n",
        "            if pred_books is not None:\n",
        "                hits = len(true_books & pred_books)\n",
        "                denom = min(k, len(true_books))\n",
        "                if denom > 0:\n",
        "                    hit_rate.append(hits / denom)\n",
        "\n",
        "                dcg = sum(1 / np.log2(rank + 2) for rank, book in enumerate(pred_books) if book in true_books)\n",
        "                idcg = sum(1 / np.log2(i + 2) for i in range(min(k, len(true_books))))\n",
        "                ndcg = dcg / idcg if idcg > 0 else 0\n",
        "                ndcg_scores.append(ndcg)\n",
        "\n",
        "            count+=1\n",
        "            if count==100:\n",
        "              break\n",
        "\n",
        "        return np.mean(hit_rate), np.mean(ndcg_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Kb8IMZ_UexQ"
      },
      "source": [
        "dùng map@12 làm metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6hP6YQMTC5m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "class CollaborativeRecommender:\n",
        "    def __init__(self, algo, trainset, testset):\n",
        "        self.algo = algo\n",
        "        self.trainset = trainset\n",
        "        self.testset = testset\n",
        "\n",
        "    def predict(self, user_id, k=10):\n",
        "        try:\n",
        "            # Convert raw user_id to inner user_id\n",
        "            inner_user_id = self.trainset.to_inner_uid(user_id)\n",
        "        except ValueError:\n",
        "            # User not found in trainset, return None\n",
        "            return None\n",
        "\n",
        "        # Get the list of books the user has interacted with\n",
        "        user_books = set(self.trainset.ur[inner_user_id])\n",
        "        all_books = set(self.trainset.all_items())\n",
        "        unseen_books = all_books - user_books\n",
        "        # Predict the ratings for unseen books\n",
        "        predictions = [self.algo.predict(self.trainset.to_raw_uid(inner_user_id), self.trainset.to_raw_iid(book_id)) for book_id in unseen_books]\n",
        "        # Sort the predictions by estimated rating and return the top-k books\n",
        "        top_predictions = sorted(predictions, key=lambda x: x.est, reverse=True)[:k]\n",
        "        top_books = [pred.iid for pred in top_predictions]\n",
        "        return top_books\n",
        "    def evaluate(self, k=12):  # Evaluate at k=12 for MAP@12\n",
        "        predictions = self.algo.test(self.testset)\n",
        "        average_precisions = []\n",
        "\n",
        "        for user_id in set(pred.uid for pred in predictions):\n",
        "            try:\n",
        "                inner_user_id = self.trainset.to_inner_uid(user_id)\n",
        "            except ValueError:\n",
        "                continue  # Skip users not in the training set\n",
        "\n",
        "            pred_books = self.predict(user_id, k)  # Get top-k predictions\n",
        "            true_books = set(book_id for (uid, book_id, _) in self.testset if uid == user_id)\n",
        "\n",
        "            # Create binary relevance labels for each prediction (1 if relevant, 0 if not)\n",
        "            relevance_labels = [1 if book in true_books else 0 for book in pred_books]\n",
        "\n",
        "            # If there are no relevant books, skip the calculation\n",
        "            if sum(relevance_labels) > 0:\n",
        "                average_precision = average_precision_score(relevance_labels, [pred.est for pred in predictions if pred.uid == user_id][:k])\n",
        "                average_precisions.append(average_precision)\n",
        "\n",
        "        # Return the mean of the average precision scores\n",
        "        return np.mean(average_precisions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z10d_ThzUL-j"
      },
      "outputs": [],
      "source": [
        "collab_recommender = CollaborativeRecommender(algo, trainset, testset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BysBsOvBUBSj",
        "outputId": "c6809bdf-6109-48f7-e5d7-be56cd3b6c91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hit Rate: 0.0110\n",
            "NDCG: 0.0117\n",
            "Elapsed time: 62.6926 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "hit_rate, ndcg = collab_recommender.evaluate(k=10)\n",
        "print(f\"Hit Rate: {hit_rate:.4f}\")\n",
        "print(f\"NDCG: {ndcg:.4f}\")\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(f\"Elapsed time: {elapsed_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcJgkkERM0ql",
        "outputId": "8a741d4d-4f46-4366-9cf5-28d05409b52e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (0.20.0)\n"
          ]
        }
      ],
      "source": [
        "pip install prometheus-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PYKekkblYL8",
        "outputId": "72422656-8f34-4c3f-ac1c-45d17d5a8c47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: prometheus_client\n",
            "Version: 0.20.0\n",
            "Summary: Python client for the Prometheus monitoring system.\n",
            "Home-page: https://github.com/prometheus/client_python\n",
            "Author: Brian Brazil\n",
            "Author-email: brian.brazil@robustperception.io\n",
            "License: Apache Software License 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: \n",
            "Required-by: jupyter-server, notebook\n"
          ]
        }
      ],
      "source": [
        "!pip show prometheus-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRAW2SSPlgW9",
        "outputId": "15a2b229-4f62-45c8-f337-857e14d9b49f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzuvZ5QllpnN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
